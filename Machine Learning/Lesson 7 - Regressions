# Lesson 7 - Regressions

- Continuous vs  Discrete
    - go with continuous when there is an order like age or income
    - examples of discrete would be phone, email

- Equation for Regression line is the equation of a line:
    - y = ax + b
        - y -> target
        - a -> slope
        - b -> intercept
        - x -> input

- r-squared score (reg.score)
    - always find score in the test data
    - the higher the better

- error (in the net worth x age example) is (actual net worth) - (predicted net worth)

- how to minimize the error:
    - the sum of all errors: not good because negative errors can cancel positive errors;
    - the sum of absolute errors: would work 
    - the sum of square error: would work

- the best regressions is the one that minimizes:
    - sum (actual - predicted)2
    - algorithms for it:
        - ordinary least squares (OLS): used by sklearn
        - gradient descent
    - there can be multiple lines that minimize sum|errors|, but only one line will minimize sum error2 (L7Q27)

- r2 (r squared) answers the question:
    - how much of my change in the output (y) is explained by the change in my input (x)
    - 0.0 < r2 < 1.0

|property   |supervised classification|regression|
|output type| discrete (class labels)|continuous (number)|
|what are you trying to find ?|decision boundary|best fit line|
|evaluation |accuracy|sum of squared error or r squared|

- Multi-variate Regression

